{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3420ec",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) and Relation Extraction on Drug-Related Adverse Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f4675",
   "metadata": {},
   "source": [
    "Named entity recognition (NER) is a subtask of natural language processing (NLP) that involves identifying and classifying named entities in text into predefined categories such as person names, organization names, locations, dates, and other types of entities. The purpose of NER is to automatically extract structured information from unstructured text, which can be used for various applications such as information retrieval, question answering, machine translation, and more.\n",
    "\n",
    "For example, given the sentence \"John works at Google in New York City\", a NER system would identify the named entities \"John\" as a person name, \"Google\" as an organization name, and \"New York City\" as a location.\n",
    "\n",
    "NER is typically performed using machine learning techniques such as rule-based systems, statistical models, and deep learning models. These models are trained on annotated datasets, where human annotators have labeled the named entities in the text. The accuracy of a NER system is measured by its precision (the percentage of correctly labeled named entities among all detected entities) and recall (the percentage of correctly labeled named entities among all actual named entities in the text).\n",
    "\n",
    "Named entity recognition (NER) can be used for identifying and extracting adverse drug effects (ADEs) from unstructured text data such as electronic health records, medical literature, and social media. ADEs are unwanted or harmful reactions that can occur as a result of taking a particular medication.\n",
    "\n",
    "NER can help to automate the process of identifying and extracting ADEs from large volumes of text, which is important because manually reviewing and annotating this data can be time-consuming and costly. NER can also improve the accuracy and completeness of ADE detection, by ensuring that all relevant information is captured and categorized correctly.\n",
    "\n",
    "Furthermore, NER can help to identify and extract specific information about the ADEs, such as the name of the drug, the type of reaction, the severity, and the time frame in which the reaction occurred. This information can be used to better understand the safety profile of medications, to identify potential drug interactions or contraindications, and to improve the overall management of patient care.\n",
    "\n",
    "Overall, using NER for ADE detection can provide significant benefits for both patients and healthcare providers, by enabling more efficient and accurate identification of adverse drug reactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f289d98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /home/diana/anaconda3/lib/python3.9/site-packages (4.26.0)\n",
      "Requirement already satisfied: datasets in /home/diana/anaconda3/lib/python3.9/site-packages (2.10.1)\n",
      "Requirement already satisfied: evaluate in /home/diana/anaconda3/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/diana/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/diana/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/diana/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/diana/.local/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/diana/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in /home/diana/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/diana/anaconda3/lib/python3.9/site-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/diana/.local/lib/python3.9/site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: filelock in /home/diana/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: xxhash in /home/diana/anaconda3/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/diana/anaconda3/lib/python3.9/site-packages (from datasets) (2022.7.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/diana/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: aiohttp in /home/diana/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/diana/anaconda3/lib/python3.9/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: multiprocess in /home/diana/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: responses<0.19 in /home/diana/anaconda3/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pandas in /home/diana/anaconda3/lib/python3.9/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/diana/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/diana/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/diana/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/diana/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/diana/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/diana/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/diana/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/diana/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/diana/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/diana/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/diana/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/diana/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/diana/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/diana/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a39d2284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved to /home/diana/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "067cac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset wnut_17 (/home/diana/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce5b79a38044e6e8aa62f02a15bbe25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "wnut = load_dataset(\"wnut_17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3f82503e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags'],\n",
       "    num_rows: 3394\n",
       "})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "26143e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['@paulwalk',\n",
       "  'It',\n",
       "  \"'s\",\n",
       "  'the',\n",
       "  'view',\n",
       "  'from',\n",
       "  'where',\n",
       "  'I',\n",
       "  \"'m\",\n",
       "  'living',\n",
       "  'for',\n",
       "  'two',\n",
       "  'weeks',\n",
       "  '.',\n",
       "  'Empire',\n",
       "  'State',\n",
       "  'Building',\n",
       "  '=',\n",
       "  'ESB',\n",
       "  '.',\n",
       "  'Pretty',\n",
       "  'bad',\n",
       "  'storm',\n",
       "  'here',\n",
       "  'last',\n",
       "  'evening',\n",
       "  '.'],\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5ba56921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-corporation',\n",
       " 'I-corporation',\n",
       " 'B-creative-work',\n",
       " 'I-creative-work',\n",
       " 'B-group',\n",
       " 'I-group',\n",
       " 'B-location',\n",
       " 'I-location',\n",
       " 'B-person',\n",
       " 'I-person',\n",
       " 'B-product',\n",
       " 'I-product']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Each number in ner_tags represents an entity. \n",
    "#Convert the numbers to their label names to find out what the entities are:\n",
    "\n",
    "label_list = wnut[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8522c7",
   "metadata": {},
   "source": [
    "The letter that prefixes each ner_tag indicates the token position of the entity:\n",
    "\n",
    "B- indicates the beginning of an entity.\n",
    "\n",
    "I- indicates a token is contained inside the same entity (for example, the State token is a part of an entity like Empire State Building).|\n",
    "\n",
    "0 indicates the token doesn’t correspond to any entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "573e64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DistilBERT tokenizer to preprocess the tokens field\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ccb2b59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '@',\n",
       " 'paul',\n",
       " '##walk',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'the',\n",
       " 'view',\n",
       " 'from',\n",
       " 'where',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'living',\n",
       " 'for',\n",
       " 'two',\n",
       " 'weeks',\n",
       " '.',\n",
       " 'empire',\n",
       " 'state',\n",
       " 'building',\n",
       " '=',\n",
       " 'es',\n",
       " '##b',\n",
       " '.',\n",
       " 'pretty',\n",
       " 'bad',\n",
       " 'storm',\n",
       " 'here',\n",
       " 'last',\n",
       " 'evening',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You’ll need to set is_split_into_words=True to tokenize the words into subwords.\n",
    "\n",
    "example = wnut[\"train\"][0]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39611e88",
   "metadata": {},
   "source": [
    "However, this adds some special tokens [CLS] and [SEP] and the subword tokenization creates a mismatch between the input and labels. A single word corresponding to a single label may now be split into two subwords. You’ll need to realign the tokens and labels by:\n",
    "\n",
    "Mapping all tokens to their corresponding word with the word_ids method.\n",
    "Assigning the label -100 to the special tokens [CLS] and [SEP] so they’re ignored by the PyTorch loss function.\n",
    "Only labeling the first token of a given word. Assign -100 to other subtokens from the same word.\n",
    "Here is how you can create a function to realign the tokens and labels, and truncate sequences to be no longer than DistilBERT’s maximum input length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8c1c6186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "61571c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/diana/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9/cache-2dfcf1e7948a3d25.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/diana/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9/cache-609269d2217156c8.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1009\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1287\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_wnut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e3dde04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/diana/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9/cache-35243a498ac41966.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/diana/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9/cache-806c272e74750a65.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/diana/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9/cache-609269d2217156c8.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e5ec8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fd1ce7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: seqeval in /home/diana/anaconda3/lib/python3.9/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/diana/.local/lib/python3.9/site-packages (from seqeval) (1.24.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/diana/anaconda3/lib/python3.9/site-packages (from seqeval) (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/diana/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/diana/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/diana/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval) (1.9.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval\n",
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b63a86bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b8858a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-corporation\",\n",
    "    2: \"I-corporation\",\n",
    "    3: \"B-creative-work\",\n",
    "    4: \"I-creative-work\",\n",
    "    5: \"B-group\",\n",
    "    6: \"I-group\",\n",
    "    7: \"B-location\",\n",
    "    8: \"I-location\",\n",
    "    9: \"B-person\",\n",
    "    10: \"I-person\",\n",
    "    11: \"B-product\",\n",
    "    12: \"I-product\",\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-corporation\": 1,\n",
    "    \"I-corporation\": 2,\n",
    "    \"B-creative-work\": 3,\n",
    "    \"I-creative-work\": 4,\n",
    "    \"B-group\": 5,\n",
    "    \"I-group\": 6,\n",
    "    \"B-location\": 7,\n",
    "    \"I-location\": 8,\n",
    "    \"B-person\": 9,\n",
    "    \"I-person\": 10,\n",
    "    \"B-product\": 11,\n",
    "    \"I-product\": 12,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8c09fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 16\n",
    "num_train_epochs = 3\n",
    "num_train_steps = (len(tokenized_wnut[\"train\"]) // batch_size) * num_train_epochs\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    "    num_warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7297ea4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForTokenClassification: ['vocab_layer_norm', 'activation_13', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_139', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForTokenClassification\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=13, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7f64e74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_wnut[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_wnut[\"validation\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "443c973d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a556df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cc2ef73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Looks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mcheck_git_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             lfs_version = run_subprocess(\n\u001b[0m\u001b[1;32m    586\u001b[0m                 \u001b[0;34m\"git-lfs --version\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/huggingface_hub/utils/_subprocess.py\u001b[0m in \u001b[0;36mrun_subprocess\u001b[0;34m(command, folder, check, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     return subprocess.run(\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1820\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1821\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1822\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'git-lfs'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12205/2655792106.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_callbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPushToHubCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m push_to_hub_callback = PushToHubCallback(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"my_awesome_wnut_model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/keras_callbacks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, save_strategy, save_steps, tokenizer, hub_model_id, hub_token, checkpoint, **model_card_args)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_model_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub_model_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mcreate_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_model_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRepository\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclone_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_model_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhub_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             )\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, token, git_user, git_email, revision, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_git_versions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36mcheck_git_versions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    587\u001b[0m             ).stdout.strip()\n\u001b[1;32m    588\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m    590\u001b[0m                 \u001b[0;34m\"Looks like you do not have git-lfs installed, please install.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0;34m\" You can install from https://git-lfs.github.com/.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Looks like you do not have git-lfs installed, please install. You can install from https://git-lfs.github.com/. Then run `git lfs install` (you only have to do this once)."
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"my_awesome_wnut_model\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4db2cf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 00:50:41.133397: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 717, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "\n",
      "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16,) + inhomogeneous part.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/datasets/utils/tf_utils.py\", line 104, in np_get_batch\n",
      "    batch = collate_fn(batch, **collate_fn_args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 43, in __call__\n",
      "    return self.tf_call(features)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 347, in tf_call\n",
      "    batch = self.tokenizer.pad(\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3020, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 210, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 733, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "\n",
      "\n",
      "2023-03-04 00:50:41.139337: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 717, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "\n",
      "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16,) + inhomogeneous part.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/datasets/utils/tf_utils.py\", line 104, in np_get_batch\n",
      "    batch = collate_fn(batch, **collate_fn_args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 43, in __call__\n",
      "    return self.tf_call(features)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 347, in tf_call\n",
      "    batch = self.tokenizer.pad(\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3020, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 210, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 733, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "\n",
      "\n",
      "2023-03-04 00:50:41.144164: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 717, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "\n",
      "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16,) + inhomogeneous part.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/datasets/utils/tf_utils.py\", line 104, in np_get_batch\n",
      "    batch = collate_fn(batch, **collate_fn_args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 43, in __call__\n",
      "    return self.tf_call(features)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 347, in tf_call\n",
      "    batch = self.tokenizer.pad(\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3020, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 210, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 733, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "\n",
      "\n",
      "2023-03-04 00:50:41.151199: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 717, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "\n",
      "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16,) + inhomogeneous part.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/datasets/utils/tf_utils.py\", line 104, in np_get_batch\n",
      "    batch = collate_fn(batch, **collate_fn_args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 43, in __call__\n",
      "    return self.tf_call(features)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 347, in tf_call\n",
      "    batch = self.tokenizer.pad(\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3020, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 210, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 733, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "\n",
      "\n",
      "2023-03-04 00:50:41.161448: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 717, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "\n",
      "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16,) + inhomogeneous part.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/datasets/utils/tf_utils.py\", line 104, in np_get_batch\n",
      "    batch = collate_fn(batch, **collate_fn_args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 43, in __call__\n",
      "    return self.tf_call(features)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 347, in tf_call\n",
      "    batch = self.tokenizer.pad(\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3020, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 210, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 733, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "\n",
      "\n",
      "2023-03-04 00:50:41.167891: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 717, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "\n",
      "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16,) + inhomogeneous part.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/datasets/utils/tf_utils.py\", line 104, in np_get_batch\n",
      "    batch = collate_fn(batch, **collate_fn_args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 43, in __call__\n",
      "    return self.tf_call(features)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 347, in tf_call\n",
      "    batch = self.tokenizer.pad(\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3020, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 210, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 733, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "\n",
      "\n",
      "2023-03-04 00:50:41.174642: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 717, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "\n",
      "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16,) + inhomogeneous part.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/datasets/utils/tf_utils.py\", line 104, in np_get_batch\n",
      "    batch = collate_fn(batch, **collate_fn_args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 43, in __call__\n",
      "    return self.tf_call(features)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 347, in tf_call\n",
      "    batch = self.tokenizer.pad(\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3020, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 210, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 733, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "\n",
      "\n",
      "2023-03-04 00:50:41.179402: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 717, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "\n",
      "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16,) + inhomogeneous part.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/datasets/utils/tf_utils.py\", line 104, in np_get_batch\n",
      "    batch = collate_fn(batch, **collate_fn_args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 43, in __call__\n",
      "    return self.tf_call(features)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 347, in tf_call\n",
      "    batch = self.tokenizer.pad(\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3020, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 210, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 733, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "\n",
      "\n",
      "2023-03-04 00:50:41.181514: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 717, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "\n",
      "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16,) + inhomogeneous part.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/datasets/utils/tf_utils.py\", line 104, in np_get_batch\n",
      "    batch = collate_fn(batch, **collate_fn_args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 43, in __call__\n",
      "    return self.tf_call(features)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 347, in tf_call\n",
      "    batch = self.tokenizer.pad(\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3020, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 210, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 733, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "\n",
      "\n",
      "2023-03-04 00:50:41.183372: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 717, in convert_to_tensors\n",
      "    tensor = as_tensor(value)\n",
      "\n",
      "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16,) + inhomogeneous part.\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/datasets/utils/tf_utils.py\", line 104, in np_get_batch\n",
      "    batch = collate_fn(batch, **collate_fn_args)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 43, in __call__\n",
      "    return self.tf_call(features)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 347, in tf_call\n",
      "    batch = self.tokenizer.pad(\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3020, in pad\n",
      "    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 210, in __init__\n",
      "    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n",
      "\n",
      "  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 733, in convert_to_tensors\n",
      "    raise ValueError(\n",
      "\n",
      "ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\nTraceback (most recent call last):\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 717, in convert_to_tensors\n    tensor = as_tensor(value)\n\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16,) + inhomogeneous part.\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/datasets/utils/tf_utils.py\", line 104, in np_get_batch\n    batch = collate_fn(batch, **collate_fn_args)\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 43, in __call__\n    return self.tf_call(features)\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 347, in tf_call\n    batch = self.tokenizer.pad(\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3020, in pad\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 210, in __init__\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 733, in convert_to_tensors\n    raise ValueError(\n\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_42823]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12205/2823156105.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_train_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_validation_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\nTraceback (most recent call last):\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 717, in convert_to_tensors\n    tensor = as_tensor(value)\n\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16,) + inhomogeneous part.\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/datasets/utils/tf_utils.py\", line 104, in np_get_batch\n    batch = collate_fn(batch, **collate_fn_args)\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 43, in __call__\n    return self.tf_call(features)\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 347, in tf_call\n    batch = self.tokenizer.pad(\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3020, in pad\n    return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 210, in __init__\n    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)\n\n  File \"/home/diana/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 733, in convert_to_tensors\n    raise ValueError(\n\nValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_42823]"
     ]
    }
   ],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0850b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The Golden State Warriors are an American professional basketball team based in San Francisco.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "eb9850b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at stevhliu/my_awesome_wnut_model were not used when initializing TFDistilBertForTokenClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at stevhliu/my_awesome_wnut_model and are newly initialized: ['dropout_179']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-location',\n",
       "  'score': 0.21302876,\n",
       "  'index': 1,\n",
       "  'word': 'the',\n",
       "  'start': 0,\n",
       "  'end': 3},\n",
       " {'entity': 'B-location',\n",
       "  'score': 0.4676814,\n",
       "  'index': 2,\n",
       "  'word': 'golden',\n",
       "  'start': 4,\n",
       "  'end': 10},\n",
       " {'entity': 'B-location',\n",
       "  'score': 0.3126429,\n",
       "  'index': 3,\n",
       "  'word': 'state',\n",
       "  'start': 11,\n",
       "  'end': 16},\n",
       " {'entity': 'B-location',\n",
       "  'score': 0.20545056,\n",
       "  'index': 4,\n",
       "  'word': 'warriors',\n",
       "  'start': 17,\n",
       "  'end': 25},\n",
       " {'entity': 'B-location',\n",
       "  'score': 0.57780087,\n",
       "  'index': 13,\n",
       "  'word': 'san',\n",
       "  'start': 80,\n",
       "  'end': 83},\n",
       " {'entity': 'B-location',\n",
       "  'score': 0.55975974,\n",
       "  'index': 14,\n",
       "  'word': 'francisco',\n",
       "  'start': 84,\n",
       "  'end': 93}]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"ner\", model=\"stevhliu/my_awesome_wnut_model\")\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b1ea3900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_wnut_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ab034b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at stevhliu/my_awesome_wnut_model were not used when initializing TFDistilBertForTokenClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForTokenClassification were not initialized from the model checkpoint at stevhliu/my_awesome_wnut_model and are newly initialized: ['dropout_199']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForTokenClassification\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\"stevhliu/my_awesome_wnut_model\")\n",
    "logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2979d3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-location',\n",
       " 'B-location',\n",
       " 'B-location',\n",
       " 'B-location',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-location',\n",
       " 'B-location',\n",
       " 'O',\n",
       " 'B-location']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_class_ids = tf.math.argmax(logits, axis=-1)\n",
    "predicted_token_class = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]\n",
    "predicted_token_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7882d43",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "164b0c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-datasets --quiet\n",
    "!pip install pydot --quiet\n",
    "!pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ad8c7174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: fastparquet in /home/diana/anaconda3/lib/python3.9/site-packages (2023.2.0)\n",
      "Requirement already satisfied: cramjam>=2.3 in /home/diana/anaconda3/lib/python3.9/site-packages (from fastparquet) (2.6.2)\n",
      "Requirement already satisfied: fsspec in /home/diana/anaconda3/lib/python3.9/site-packages (from fastparquet) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/diana/.local/lib/python3.9/site-packages (from fastparquet) (1.24.1)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /home/diana/anaconda3/lib/python3.9/site-packages (from fastparquet) (1.5.3)\n",
      "Requirement already satisfied: packaging in /home/diana/.local/lib/python3.9/site-packages (from fastparquet) (23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/diana/anaconda3/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/diana/anaconda3/lib/python3.9/site-packages (from pandas>=1.5.0->fastparquet) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/diana/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "import sklearn as sk\n",
    "import os\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ad280f",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d13744",
   "metadata": {},
   "source": [
    "### Dugs and Adverse Effects Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99fceb11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PubMed-ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Adverse-Effect</th>\n",
       "      <th>Begin Effect Offset</th>\n",
       "      <th>End Effect Offset</th>\n",
       "      <th>Drug</th>\n",
       "      <th>Begin Drug Effect Offset</th>\n",
       "      <th>End Drug Effect Offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10030778</td>\n",
       "      <td>Intravenous azithromycin-induced ototoxicity.</td>\n",
       "      <td>ototoxicity</td>\n",
       "      <td>43</td>\n",
       "      <td>54</td>\n",
       "      <td>azithromycin</td>\n",
       "      <td>22</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10048291</td>\n",
       "      <td>Immobilization, while Paget's bone disease was...</td>\n",
       "      <td>increased calcium-release</td>\n",
       "      <td>960</td>\n",
       "      <td>985</td>\n",
       "      <td>dihydrotachysterol</td>\n",
       "      <td>908</td>\n",
       "      <td>926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10048291</td>\n",
       "      <td>Unaccountable severe hypercalcemia in a patien...</td>\n",
       "      <td>hypercalcemia</td>\n",
       "      <td>31</td>\n",
       "      <td>44</td>\n",
       "      <td>dihydrotachysterol</td>\n",
       "      <td>94</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10082597</td>\n",
       "      <td>METHODS: We report two cases of pseudoporphyri...</td>\n",
       "      <td>pseudoporphyria</td>\n",
       "      <td>620</td>\n",
       "      <td>635</td>\n",
       "      <td>naproxen</td>\n",
       "      <td>646</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10082597</td>\n",
       "      <td>METHODS: We report two cases of pseudoporphyri...</td>\n",
       "      <td>pseudoporphyria</td>\n",
       "      <td>620</td>\n",
       "      <td>635</td>\n",
       "      <td>oxaprozin</td>\n",
       "      <td>659</td>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PubMed-ID                                           Sentence  \\\n",
       "0   10030778      Intravenous azithromycin-induced ototoxicity.   \n",
       "1   10048291  Immobilization, while Paget's bone disease was...   \n",
       "2   10048291  Unaccountable severe hypercalcemia in a patien...   \n",
       "3   10082597  METHODS: We report two cases of pseudoporphyri...   \n",
       "4   10082597  METHODS: We report two cases of pseudoporphyri...   \n",
       "\n",
       "              Adverse-Effect  Begin Effect Offset  End Effect Offset  \\\n",
       "0                ototoxicity                   43                 54   \n",
       "1  increased calcium-release                  960                985   \n",
       "2              hypercalcemia                   31                 44   \n",
       "3            pseudoporphyria                  620                635   \n",
       "4            pseudoporphyria                  620                635   \n",
       "\n",
       "                 Drug  Begin Drug Effect Offset  End Drug Effect Offset  \n",
       "0        azithromycin                        22                      34  \n",
       "1  dihydrotachysterol                       908                     926  \n",
       "2  dihydrotachysterol                        94                     112  \n",
       "3            naproxen                       646                     654  \n",
       "4           oxaprozin                       659                     668  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read DRUG-AE.rel file\n",
    "# Provides relations between drugs and adverse effects\n",
    "\n",
    "df_adverse_effect = pd.read_csv(\"DRUG-AE.rel\", header=None, delimiter=\"|\")\n",
    "df_adverse_effect.rename(columns = {0:\"PubMed-ID\", \n",
    "                                    1:\"Sentence\",\n",
    "                                    2:\"Adverse-Effect\", \n",
    "                                    3:\"Begin Effect Offset\", \n",
    "                                    4:\"End Effect Offset\", \n",
    "                                    5:\"Drug\", \n",
    "                                    6:\"Begin Drug Effect Offset\",\n",
    "                                    7:\"End Drug Effect Offset\"},inplace = True)\n",
    "\n",
    "df_adverse_effect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b213e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6821, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_adverse_effect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51ce46e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6821 entries, 0 to 6820\n",
      "Data columns (total 8 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   PubMed-ID                 6821 non-null   int64 \n",
      " 1   Sentence                  6821 non-null   object\n",
      " 2   Adverse-Effect            6821 non-null   object\n",
      " 3   Begin Effect Offset       6821 non-null   int64 \n",
      " 4   End Effect Offset         6821 non-null   int64 \n",
      " 5   Drug                      6821 non-null   object\n",
      " 6   Begin Drug Effect Offset  6821 non-null   int64 \n",
      " 7   End Drug Effect Offset    6821 non-null   int64 \n",
      "dtypes: int64(5), object(3)\n",
      "memory usage: 426.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_adverse_effect.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3877d93",
   "metadata": {},
   "source": [
    "### Drugs and Dosages Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d49cf722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PubMed-ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Dose</th>\n",
       "      <th>Begin Dose Offset</th>\n",
       "      <th>End Dose Offset</th>\n",
       "      <th>Drug</th>\n",
       "      <th>Begin Drug Dose Offset</th>\n",
       "      <th>End Drug Dose Offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10327035</td>\n",
       "      <td>An episode of subacute encephalopathy after th...</td>\n",
       "      <td>1500 mg/m2</td>\n",
       "      <td>230</td>\n",
       "      <td>240</td>\n",
       "      <td>methotrexate</td>\n",
       "      <td>216</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10452772</td>\n",
       "      <td>She continued to receive regular insulin 4 tim...</td>\n",
       "      <td>4 times per day</td>\n",
       "      <td>1473</td>\n",
       "      <td>1488</td>\n",
       "      <td>insulin</td>\n",
       "      <td>1465</td>\n",
       "      <td>1472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10458196</td>\n",
       "      <td>A 5-month-old infant became lethargic and poor...</td>\n",
       "      <td>1 drop</td>\n",
       "      <td>522</td>\n",
       "      <td>528</td>\n",
       "      <td>brimonidine</td>\n",
       "      <td>532</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10667036</td>\n",
       "      <td>The presented patient was treated with 200 mg ...</td>\n",
       "      <td>200 mg</td>\n",
       "      <td>389</td>\n",
       "      <td>395</td>\n",
       "      <td>TCA</td>\n",
       "      <td>396</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10698143</td>\n",
       "      <td>Central nervous system manifestations of an ib...</td>\n",
       "      <td>overdose</td>\n",
       "      <td>64</td>\n",
       "      <td>72</td>\n",
       "      <td>ibuprofen</td>\n",
       "      <td>54</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PubMed-ID                                           Sentence  \\\n",
       "0   10327035  An episode of subacute encephalopathy after th...   \n",
       "1   10452772  She continued to receive regular insulin 4 tim...   \n",
       "2   10458196  A 5-month-old infant became lethargic and poor...   \n",
       "3   10667036  The presented patient was treated with 200 mg ...   \n",
       "4   10698143  Central nervous system manifestations of an ib...   \n",
       "\n",
       "              Dose  Begin Dose Offset  End Dose Offset          Drug  \\\n",
       "0       1500 mg/m2                230              240  methotrexate   \n",
       "1  4 times per day               1473             1488       insulin   \n",
       "2           1 drop                522              528   brimonidine   \n",
       "3           200 mg                389              395           TCA   \n",
       "4         overdose                 64               72     ibuprofen   \n",
       "\n",
       "   Begin Drug Dose Offset  End Drug Dose Offset  \n",
       "0                     216                   228  \n",
       "1                    1465                  1472  \n",
       "2                     532                   543  \n",
       "3                     396                   399  \n",
       "4                      54                    63  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read DRUG-DOSE.rel file\n",
    "# Provides relations between drugs and dosages\n",
    "\n",
    "df_dosage = pd.read_csv(\"DRUG-DOSE.rel\", header=None, delimiter=\"|\")\n",
    "df_dosage.rename(columns = {0:\"PubMed-ID\", \n",
    "                                    1:\"Sentence\",\n",
    "                                    2:\"Dose\", \n",
    "                                    3:\"Begin Dose Offset\", \n",
    "                                    4:\"End Dose Offset\", \n",
    "                                    5:\"Drug\", \n",
    "                                    6:\"Begin Drug Dose Offset\",\n",
    "                                    7:\"End Drug Dose Offset\"},inplace = True)\n",
    "\n",
    "df_dosage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbec6770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(279, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dosage.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef1ab6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 279 entries, 0 to 278\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   PubMed-ID               279 non-null    int64 \n",
      " 1   Sentence                279 non-null    object\n",
      " 2   Dose                    279 non-null    object\n",
      " 3   Begin Dose Offset       279 non-null    int64 \n",
      " 4   End Dose Offset         279 non-null    int64 \n",
      " 5   Drug                    279 non-null    object\n",
      " 6   Begin Drug Dose Offset  279 non-null    int64 \n",
      " 7   End Drug Dose Offset    279 non-null    int64 \n",
      "dtypes: int64(5), object(3)\n",
      "memory usage: 17.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_dosage.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092cc37",
   "metadata": {},
   "source": [
    "### Drugs and No Adverse Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf9ea45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6460590 NEG Clioquinol intoxication occurring ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8600337 NEG \"Retinoic acid syndrome\" was preve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8402502 NEG BACKGROUND: External beam radiatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8700794 NEG Although the enuresis ceased, she ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17662448 NEG A 42-year-old woman had uneventfu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  6460590 NEG Clioquinol intoxication occurring ...\n",
       "1  8600337 NEG \"Retinoic acid syndrome\" was preve...\n",
       "2  8402502 NEG BACKGROUND: External beam radiatio...\n",
       "3  8700794 NEG Although the enuresis ceased, she ...\n",
       "4  17662448 NEG A 42-year-old woman had uneventfu..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neg = pd.read_csv(\"ADE-NEG.txt\", header=None, delimiter=\"|\")\n",
    "df_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d10beffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PubMed-ID</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6460590</td>\n",
       "      <td>NEG Clioquinol intoxication occurring in the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8600337</td>\n",
       "      <td>NEG \"Retinoic acid syndrome\" was prevented wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8402502</td>\n",
       "      <td>NEG BACKGROUND: External beam radiation therap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8700794</td>\n",
       "      <td>NEG Although the enuresis ceased, she develope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17662448</td>\n",
       "      <td>NEG A 42-year-old woman had uneventful bilater...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16690</th>\n",
       "      <td>946400</td>\n",
       "      <td>NEG At autopsy, the liver was found to be smal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16691</th>\n",
       "      <td>16416684</td>\n",
       "      <td>NEG Physical exam revealed a patient with apha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16692</th>\n",
       "      <td>7351000</td>\n",
       "      <td>NEG At the time when the leukemia appeared sev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16693</th>\n",
       "      <td>19769520</td>\n",
       "      <td>NEG The American Society for Regional Anesthes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16694</th>\n",
       "      <td>11300139</td>\n",
       "      <td>NEG Concomitant administration of estradiol re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16695 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PubMed-ID                                           Sentence\n",
       "0       6460590  NEG Clioquinol intoxication occurring in the t...\n",
       "1       8600337  NEG \"Retinoic acid syndrome\" was prevented wit...\n",
       "2       8402502  NEG BACKGROUND: External beam radiation therap...\n",
       "3       8700794  NEG Although the enuresis ceased, she develope...\n",
       "4      17662448  NEG A 42-year-old woman had uneventful bilater...\n",
       "...         ...                                                ...\n",
       "16690    946400  NEG At autopsy, the liver was found to be smal...\n",
       "16691  16416684  NEG Physical exam revealed a patient with apha...\n",
       "16692   7351000  NEG At the time when the leukemia appeared sev...\n",
       "16693  19769520  NEG The American Society for Regional Anesthes...\n",
       "16694  11300139  NEG Concomitant administration of estradiol re...\n",
       "\n",
       "[16695 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neg[\"PubMed-ID\"] = df_neg[0].str.split(' ').str[0]\n",
    "df_neg['Sentence'] = df_neg[0].str.split(n=1).str[1]\n",
    "df_neg.drop(columns=df_neg.columns[0], axis=1, inplace=True)\n",
    "df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5333630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16695, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c855ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16695 entries, 0 to 16694\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   PubMed-ID  16695 non-null  object\n",
      " 1   Sentence   16695 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 261.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_neg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a5d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42280630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f58681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bea1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe1a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a898d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb32031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab92a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d89f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3cd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c7087f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef1d218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15197df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e00ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e770e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b74e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82effa48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26c3dc3c",
   "metadata": {},
   "source": [
    "# Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f54652c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78ea689c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset ade_corpus_v2 (/home/diana/.cache/huggingface/datasets/ade_corpus_v2/Ade_corpus_v2_drug_ade_relation/1.0.0/940d61334dbfac6b01ac5d00286a2122608b8dc79706ee7e9206a1edb172c559)\n"
     ]
    }
   ],
   "source": [
    "ds=load_dataset('ade_corpus_v2', 'Ade_corpus_v2_drug_ade_relation', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96b914f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'drug', 'effect', 'indexes'],\n",
       "    num_rows: 6821\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60ebcf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Intravenous azithromycin-induced ototoxicity.',\n",
       "  \"Immobilization, while Paget's bone disease was present, and perhaps enhanced activation of dihydrotachysterol by rifampicin, could have led to increased calcium-release into the circulation.\"],\n",
       " 'drug': ['azithromycin', 'dihydrotachysterol'],\n",
       " 'effect': ['ototoxicity', 'increased calcium-release'],\n",
       " 'indexes': [{'drug': {'start_char': [12], 'end_char': [24]},\n",
       "   'effect': {'start_char': [33], 'end_char': [44]}},\n",
       "  {'drug': {'start_char': [91], 'end_char': [109]},\n",
       "   'effect': {'start_char': [143], 'end_char': [168]}}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dc38c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 16:10:37.860844: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/diana/tensorflow_datasets/ade_corpus_v2/ade_corpus_v2_drug_ade_relation/1.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diana/anaconda3/lib/python3.9/site-packages/datasets/load.py:1736: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
      "To ignore verifications, you can pass `verification_mode='no_checks'` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'full' is not a valid VerificationMode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12205/3246287089.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'huggingface:ade_corpus_v2/Ade_corpus_v2_drug_ade_relation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_datasets/core/logging/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m       \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[1;32m    626\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m           self._download_and_prepare(\n\u001b[0m\u001b[1;32m    629\u001b[0m               \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m               \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1434\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m         \u001b[0moptional_pipeline_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m       split_generators = self._split_generators(  # pylint: disable=unexpected-keyword-arg\n\u001b[0m\u001b[1;32m   1437\u001b[0m           \u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptional_pipeline_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m       )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builders/huggingface_dataset_builder.py\u001b[0m in \u001b[0;36m_split_generators\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mhf_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlazy_imports_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_imports\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     hf_dataset_dict = hf_datasets.load_dataset(\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0mdownload_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_mode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREUSE_DATASET_IF_EXISTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1754\u001b[0;31m     verification_mode = VerificationMode(\n\u001b[0m\u001b[1;32m   1755\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mverification_mode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mVerificationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBASIC_CHECKS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msave_infos\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mVerificationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_CHECKS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/enum.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \"\"\"\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# simple value lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;31m# otherwise, functional API: we're creating a new Enum type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         return cls._create_(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/enum.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mve_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%r is not a valid %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mve_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                     exc = TypeError(\n",
      "\u001b[0;31mValueError\u001b[0m: 'full' is not a valid VerificationMode"
     ]
    }
   ],
   "source": [
    "builder = tfds.builder('huggingface:ade_corpus_v2/Ade_corpus_v2_drug_ade_relation')\n",
    "builder.download_and_prepare()\n",
    "ds = builder.as_dataset(split='train')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33ea5eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/diana/tensorflow_datasets/ade_corpus_v2/ade_corpus_v2_drug_ade_relation/1.0.0...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'full' is not a valid VerificationMode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12205/2343158015.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'huggingface:ade_corpus_v2/Ade_corpus_v2_drug_ade_relation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_datasets/core/logging/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m       \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    615\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m     \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_datasets/core/logging/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m       \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[1;32m    626\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m           self._download_and_prepare(\n\u001b[0m\u001b[1;32m    629\u001b[0m               \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m               \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1434\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m         \u001b[0moptional_pipeline_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m       split_generators = self._split_generators(  # pylint: disable=unexpected-keyword-arg\n\u001b[0m\u001b[1;32m   1437\u001b[0m           \u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptional_pipeline_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m       )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builders/huggingface_dataset_builder.py\u001b[0m in \u001b[0;36m_split_generators\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mhf_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlazy_imports_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_imports\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     hf_dataset_dict = hf_datasets.load_dataset(\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0mdownload_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_mode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREUSE_DATASET_IF_EXISTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1754\u001b[0;31m     verification_mode = VerificationMode(\n\u001b[0m\u001b[1;32m   1755\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mverification_mode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mVerificationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBASIC_CHECKS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msave_infos\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mVerificationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_CHECKS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/enum.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \"\"\"\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# simple value lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;31m# otherwise, functional API: we're creating a new Enum type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         return cls._create_(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/enum.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mve_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%r is not a valid %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mve_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                     exc = TypeError(\n",
      "\u001b[0;31mValueError\u001b[0m: 'full' is not a valid VerificationMode"
     ]
    }
   ],
   "source": [
    "ds = tfds.load('huggingface:ade_corpus_v2/Ade_corpus_v2_drug_ade_relation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf8af0",
   "metadata": {},
   "source": [
    "github repo\n",
    "\n",
    "tfhub\n",
    "\n",
    "person place\n",
    "relation\n",
    "\n",
    "recognize drug and effect\n",
    "what is the relationship? Find a dataset\n",
    "\n",
    "Example:\n",
    "    Mark is born\n",
    "    \n",
    "NER\n",
    "conver to bio format\n",
    "for each of the token, is the token inside(b) or outside(o)\n",
    "\n",
    "run through biobert tokenizer?\n",
    "\n",
    "string matching?\n",
    "\n",
    "NER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
